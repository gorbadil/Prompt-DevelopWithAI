# Offensive Measures

Offensive measures in prompt hacking actively test AI systems for vulnerabilities. Researchers use techniques like adversarial prompts and model probing to identify weaknesses, enabling improved defenses and highlighting potential risks in deployed AI models.

## Adversarial Prompts

Adversarial prompts are crafted to exploit AI models by triggering unintended behaviors or generating harmful outputs. These prompts are designed to test the system's robustness and security, revealing vulnerabilities that could be exploited by malicious actors. By creating adversarial prompts, researchers can assess the model's response to various inputs and identify potential weaknesses that need to be addressed.

## Model Probing

Model probing involves analyzing AI models to understand their internal mechanisms and decision-making processes. Researchers use probing techniques to investigate how models interpret prompts, make predictions, and generate outputs. By probing AI models, researchers can uncover hidden biases, vulnerabilities, and limitations that may impact the model's performance and reliability. This information helps researchers develop more effective defenses and improve the overall security of AI systems.
