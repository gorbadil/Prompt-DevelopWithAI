# Halüsinasyonlar

Büyük Dil Modeli (LLM) halüsinasyonları, genel olarak doğruluk ve gerçeklik sorunları olarak kategorize edilebilir. **Doğruluk halüsinasyonları**, modelin çıktısının sağlanan kaynaklardan veya bağlamdan sapması durumunda ortaya çıkar ve kaynak-referans sapması, bağlam geri çağırma, diyalog geçmişi yanlış yorumu ve hatalı özetleme gibi sorunları içerir. **Gerçeklik halüsinasyonları** ise yanlış veya desteklenmeyen bilgilerin üretilmesini içerir ve gerçek hatalar, varlık hataları, aşırı iddialar, doğrulanamayan ifadeler, anlamsız yanıtlar, çelişkiler ve uydurma veriler gibi durumları kapsar.

Bu halüsinasyonlar, eğitim verisi sorunları, model sınırlamaları, istemle ilgili problemler ve aşırı öğrenme gibi çeşitli nedenlerden kaynaklanır. Bu zorlukları hafifletmek için, Geri Çağırma Destekli Üretim (RAG), geliştirilmiş eğitim verisi, titiz değerlendirme, net kullanıcı iletişimi, ileri istem mühendisliği, model ince ayarı, çıktı filtreleme ve çoklu model yaklaşımları gibi stratejiler kullanılmaktadır. Alan ilerledikçe, bu halüsinasyon türlerini anlamak ve ele almak, LLM tarafından üretilen içeriğin güvenilirliğini ve güvenilirliğini artırmak için kritik önem taşımaktadır.
